{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e325c8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Security validation (CAPTCHA/2FA) detected. Please complete it manually in the browser and press Enter here...\n",
      " Login successful!\n",
      "{\n",
      "  \"Full name\": \"Harsh Singh\",\n",
      "  \"Job title\": \"Contributor @GSSoC'24 extd. | Codechef 1300+ | 5â­ - C, Problem Solving | 4â­ - C++ | MPGI'27 | Code-o-Fiesta 2.0 Runner-up\",\n",
      "  \"Company\": \"Codolio\",\n",
      "  \"Location\": \"Kanpur, Uttar Pradesh, India\",\n",
      "  \"Professional Insights\": [],\n",
      "  \"LinkedIn bio\": \"Eager to gain experience, and open to challenges. Eager to gain experience, and open to challenges.\",\n",
      "  \"Tech stack\": [],\n",
      "  \"Recent LinkedIn activity\": [\n",
      "    \"Tadaa!! ðŸ±ðŸ¬ðŸ¬ ð—¾ð˜‚ð—²ð˜€ð˜ð—¶ð—¼ð—»ð˜€ ð˜€ð—¼ð—¹ð˜ƒð—²ð—± across all platformsðŸ˜¤.This is a big milestone for me. Along the way, Iâ€™ve learned so much â€” from basic data structures to complex algorithms and clever shortcuts. More importantly, Iâ€™ve learned how to approach problems Iâ€™ve never seen before. Also,Codoliocard looks better now.Already looking forward to hitting ðŸ­ðŸ¬ðŸ¬ðŸ¬!\",\n",
      "    \"Just received the ðŸ±ðŸ¬ ð—±ð—®ð˜†ð˜€ ð—¯ð—®ð—±ð—´ð—² inLeetCode.It hasn't been my primary focus over the past few months, but as they say, better late than never. ðŸ˜…hashtag#leetcodehashtag#50days\",\n",
      "    \"ð—£ð—¿ð—¼ð—¯ð—¹ð—²ð—º-ð˜€ð—¼ð—¹ð˜ƒð—¶ð—»ð—´ ð—¶ð˜€ ð—®ð—¹ð—¹ ð—®ð—¯ð—¼ð˜‚ð˜ ð—¹ð—²ð—®ð—¿ð—»ð—¶ð—»ð—´ ð—»ð—²ð˜„ ð˜ð—µð—¶ð—»ð—´ð˜€. Here are some things that I've learnt over the past few days:\",\n",
      "    \"Was revising DSA and got a rather pleasant surprise: ðŸ±â­ ð—¶ð—» ð—£ð—¿ð—¼ð—¯ð—¹ð—²ð—º ð—¦ð—¼ð—¹ð˜ƒð—¶ð—»ð—´ onHackerRank.hashtag#HackerRankhashtag#problemsolving\",\n",
      "    \"Just earned the ð——ð—¶ð—®ð—ºð—¼ð—»ð—± ð—¦ð˜ð—¿ð—²ð—®ð—¸ ð—•ð—®ð—±ð—´ð—² atCodeChef.\"\n",
      "  ],\n",
      "  \"Job history changes\": [\n",
      "    {\n",
      "      \"raw\": \"Contributor | Contributor | GirlScript Summer of Code Â· Internship | GirlScript Summer of Code Â· Internship | Oct 2024 - Oct 2024 Â· 1 mo | Oct 2024 to Oct 2024 Â· 1 mo | Kanpur, Uttar Pradesh, India Â· Remote | Kanpur, Uttar Pradesh, India Â· Remote | Skills: | GitHub | Skills: | GitHub\",\n",
      "      \"title\": \"Contributor\",\n",
      "      \"company\": \"GirlScript Summer of Code Â· Internship\",\n",
      "      \"dates\": \"Oct 2024 to Oct 2024 Â· 1 mo\",\n",
      "      \"location\": \"Kanpur, Uttar Pradesh, India Â· Remote\"\n",
      "    }\n",
      "  ],\n",
      "  \"Company growth signals\": {\n",
      "    \"company_page\": \"https://www.linkedin.com/company/14488336/\",\n",
      "    \"followers\": \"154K  followers\",\n",
      "    \"employees_on_linkedin\": null\n",
      "  },\n",
      "  \"profile_url\": \"https://www.linkedin.com/in/harsh-o4/\"\n",
      "}\n",
      "\n",
      " Saved: linkedin_profile.json, linkedin_profile.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "EMAIL = \"akashdeepp212@gmail.com\"          \n",
    "PASSWORD = \"AkkuDeep@99\"   \n",
    "PROFILE_URL = \"https://www.linkedin.com/in/harsh-o4/\"  \n",
    "\n",
    "HEADLESS = False\n",
    "\n",
    "def build_driver() -> webdriver.Chrome:\n",
    "    chrome_options = Options()\n",
    "    if HEADLESS:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    \n",
    "    return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def wait_css(drv, selector, timeout=10):\n",
    "    return WebDriverWait(drv, timeout).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "    )\n",
    "\n",
    "def safe_text(el) -> str:\n",
    "    try:\n",
    "        return el.text.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def get_soup(drv) -> BeautifulSoup:\n",
    "    return BeautifulSoup(drv.page_source, \"lxml\")\n",
    "\n",
    "def click_if_present(drv, by, value, timeout=3) -> bool:\n",
    "    try:\n",
    "        el = WebDriverWait(drv, timeout).until(EC.element_to_be_clickable((by, value)))\n",
    "        drv.execute_script(\"arguments[0].click();\", el)\n",
    "        time.sleep(1)\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "def click_all_see_more(drv):\n",
    "    \n",
    "    patterns = [\n",
    "        (By.CLASS_NAME, \"inline-show-more-text__button\"),\n",
    "        (By.XPATH, \"//button[contains(., 'See more')]\"),\n",
    "        (By.XPATH, \"//button[contains(., 'Show all')]\"),\n",
    "        (By.CSS_SELECTOR, \"button[aria-expanded='false']\"),\n",
    "    ]\n",
    "    clicked_any = True\n",
    "\n",
    "    for _ in range(3):\n",
    "        if not clicked_any:\n",
    "            break\n",
    "        clicked_any = False\n",
    "        for by, sel in patterns:\n",
    "            try:\n",
    "                buttons = drv.find_elements(by, sel)\n",
    "            except Exception:\n",
    "                buttons = []\n",
    "            for b in buttons:\n",
    "                try:\n",
    "                    if b.is_displayed():\n",
    "                        drv.execute_script(\"arguments[0].click();\", b)\n",
    "                        time.sleep(0.5)\n",
    "                        clicked_any = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "\n",
    "# Login with manual security validation\n",
    "\n",
    "def linkedin_login(drv, email: str, password: str):\n",
    "    try:\n",
    "        drv.get(\"https://www.linkedin.com/login\")\n",
    "        wait_css(drv, \"#username\", timeout=15)\n",
    "        drv.find_element(By.ID, \"username\").send_keys(email)\n",
    "        drv.find_element(By.ID, \"password\").send_keys(password)\n",
    "        drv.find_element(By.ID, \"password\").send_keys(Keys.RETURN)\n",
    "        time.sleep(3)  \n",
    "\n",
    "        # Check for security validation (CAPTCHA)\n",
    "        try:\n",
    "            WebDriverWait(drv, 10).until(\n",
    "                EC.any_of(\n",
    "                    EC.presence_of_element_located((By.ID, \"input__email_verification_pin\")),\n",
    "                    EC.url_contains(\"checkpoint/challenge\")\n",
    "                )\n",
    "            )\n",
    "            print(\" Security validation (CAPTCHA/2FA) detected. Please complete it manually in the browser and press Enter here...\")\n",
    "            input(\"Press Enter after completing the security validation...\")\n",
    "            time.sleep(2)  \n",
    "        except TimeoutException:\n",
    "            print(\"â„¹ No security validation detected.\")\n",
    "\n",
    "        # Verify login success\n",
    "        current_url = drv.current_url\n",
    "        if \"feed\" in current_url or \"home\" in current_url:\n",
    "            print(\" Login successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\" Login failed. Current URL: {current_url}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\" Login error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def scrape_top_card(drv) -> Dict[str, Any]:\n",
    "    data = {\n",
    "        \"Full name\": \"Not found\",\n",
    "        \"Job title\": \"Not found\",\n",
    "        \"Company\": \"Not found\",\n",
    "        \"Location\": \"Not found\",\n",
    "    }\n",
    "    try:\n",
    "        wait_css(drv, \"main\", timeout=15)\n",
    "    except TimeoutException:\n",
    "        return data\n",
    "\n",
    "    click_all_see_more(drv)\n",
    "    soup = get_soup(drv)\n",
    "\n",
    "    # Full name\n",
    "    name = \"\"\n",
    "    for sel in [\n",
    "        \"h1.text-heading-xlarge\",\n",
    "        \"h1\",\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el and el.get_text(strip=True):\n",
    "            name = el.get_text(strip=True)\n",
    "            break\n",
    "    if name:\n",
    "        data[\"Full name\"] = name\n",
    "\n",
    "    # Job title (headline under name)\n",
    "    headline = \"\"\n",
    "    for sel in [\n",
    "        \"div.text-body-medium.break-words\",\n",
    "        \"div[data-view-name='profile-card'] div.inline-show-more-text\",\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el and el.get_text(strip=True):\n",
    "            headline = el.get_text(strip=True)\n",
    "            break\n",
    "    if headline:\n",
    "        data[\"Job title\"] = headline\n",
    "\n",
    "    # Location\n",
    "    location = \"\"\n",
    "    for sel in [\n",
    "        \"span.text-body-small.inline.t-black--light.break-words\",\n",
    "        \"span.inline.t-black--light\",\n",
    "    ]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el and el.get_text(strip=True):\n",
    "            location = el.get_text(strip=True)\n",
    "            break\n",
    "    if location:\n",
    "        data[\"Location\"] = location\n",
    "\n",
    "    # Company \n",
    "    company = \"\"\n",
    "    top_card = soup.select_one(\"div.pv-text-details__right-panel\")\n",
    "    if top_card:\n",
    "        a_tags = top_card.select(\"a[href*='/company/']\")\n",
    "        if a_tags:\n",
    "            company = a_tags[0].get_text(strip=True)\n",
    "    if not company:\n",
    "        a_tags = soup.select(\"a[href*='/company/']\")\n",
    "        if a_tags:\n",
    "            company = a_tags[0].get_text(strip=True)\n",
    "    if company:\n",
    "        data[\"Company\"] = company\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# About section\n",
    "\n",
    "def scrape_about(drv) -> str:\n",
    "    try:\n",
    "        drv.get(drv.current_url.split(\"?\")[0])  \n",
    "        time.sleep(1)\n",
    "        drv.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 0.35);\")\n",
    "        time.sleep(1)\n",
    "        click_all_see_more(drv)\n",
    "        soup = get_soup(drv)\n",
    "        about_block = None\n",
    "        for sel in [\n",
    "            \"section[id='about'] div.inline-show-more-text\",\n",
    "            \"section[id='about']\",\n",
    "            \"div.display-flex.ph5.pv3\",\n",
    "        ]:\n",
    "            about_block = soup.select_one(sel)\n",
    "            if about_block:\n",
    "                break\n",
    "        if about_block:\n",
    "            text = about_block.get_text(separator=\" \", strip=True)\n",
    "            return text if text else \"Not found\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"Not found\"\n",
    "\n",
    "# Professional Insights\n",
    "def scrape_professional_insights(drv) -> List[str]:\n",
    "    insights = []\n",
    "    try:\n",
    "        drv.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(1)\n",
    "        soup = get_soup(drv)\n",
    "        selectors = [\n",
    "            \"section.pv-highlights-section\",\n",
    "            \"section[id='highlight-container']\",\n",
    "            \"div.pvs-highlight__container\",\n",
    "            \"div.pv-profile-card--feature\",\n",
    "        ]\n",
    "        for sel in selectors:\n",
    "            for sec in soup.select(sel):\n",
    "                txt = sec.get_text(separator=\" \", strip=True)\n",
    "                if txt and txt not in insights:\n",
    "                    insights.append(txt)\n",
    "        return insights[:10]\n",
    "    except Exception:\n",
    "        return insights\n",
    "\n",
    "\n",
    "# Skills\n",
    "\n",
    "def scrape_skills(drv, profile_url: str) -> List[str]:\n",
    "    skills = []\n",
    "    try:\n",
    "        drv.get(profile_url.rstrip(\"/\") + \"/details/skills/\")\n",
    "        wait_css(drv, \"main\", timeout=10)\n",
    "        click_all_see_more(drv)\n",
    "        soup = get_soup(drv)\n",
    "        for sel in [\n",
    "            \"span.mr1.t-bold\",\n",
    "            \"span.pvs-list__item--one-column span[aria-hidden='true']\",\n",
    "            \"div.pvs-list__outer-container li span.mr1.t-bold\",\n",
    "        ]:\n",
    "            for el in soup.select(sel):\n",
    "                text = el.get_text(strip=True)\n",
    "                if text and text not in skills:\n",
    "                    skills.append(text)\n",
    "        return skills[:50]\n",
    "    except Exception:\n",
    "        return skills\n",
    "\n",
    "\n",
    "# Recent activity\n",
    "\n",
    "def scrape_recent_activity(drv, profile_url: str) -> List[str]:\n",
    "    items = []\n",
    "    try:\n",
    "        drv.get(profile_url.rstrip(\"/\") + \"/recent-activity/all/\")\n",
    "        wait_css(drv, \"main\", timeout=10)\n",
    "        time.sleep(2)\n",
    "        soup = get_soup(drv)\n",
    "        selectors = [\n",
    "            \"div.update-components-text.relative span[dir='ltr']\",\n",
    "            \"div.feed-shared-update-v2__description-wrapper span[dir='ltr']\",\n",
    "            \"a.app-aware-link[aria-label]\",\n",
    "        ]\n",
    "        for sel in selectors:\n",
    "            for el in soup.select(sel):\n",
    "                txt = el.get_text(strip=True)\n",
    "                if txt and txt not in items:\n",
    "                    items.append(txt)\n",
    "        return items[:10]\n",
    "    except Exception:\n",
    "        return items\n",
    "\n",
    "\n",
    "# Experience\n",
    "\n",
    "def scrape_experience(drv, profile_url: str) -> List[Dict[str, str]]:\n",
    "    jobs: List[Dict[str, str]] = []\n",
    "    try:\n",
    "        drv.get(profile_url.rstrip(\"/\") + \"/details/experience/\")\n",
    "        wait_css(drv, \"main\", timeout=10)\n",
    "        click_all_see_more(drv)\n",
    "        soup = get_soup(drv)\n",
    "        cards = soup.select(\"li.pvs-list__paged-list-item, li.pvs-list__item--line-separated\")\n",
    "        if not cards:\n",
    "            cards = soup.select(\"div.pvs-entity\")\n",
    "        for card in cards:\n",
    "            spans = [s.get_text(strip=True) for s in card.select(\"span.visually-hidden\")]\n",
    "            text = card.get_text(separator=\" | \", strip=True)\n",
    "            role = {\n",
    "                \"raw\": text[:300] if text else \"\",\n",
    "                \"title\": \"\",\n",
    "                \"company\": \"\",\n",
    "                \"dates\": \"\",\n",
    "                \"location\": \"\",\n",
    "            }\n",
    "            if spans:\n",
    "                role[\"title\"] = spans[0] if len(spans) >= 1 else \"\"\n",
    "                role[\"company\"] = spans[1] if len(spans) >= 2 else \"\"\n",
    "                role[\"dates\"] = spans[2] if len(spans) >= 3 else \"\"\n",
    "                role[\"location\"] = spans[3] if len(spans) >= 4 else \"\"\n",
    "            jobs.append(role)\n",
    "        return jobs[:30]\n",
    "    except Exception:\n",
    "        return jobs\n",
    "\n",
    "\n",
    "# Company growth signals\n",
    "\n",
    "def scrape_company_growth(drv) -> Dict[str, Any]:\n",
    "    result = {\"company_page\": None, \"followers\": None, \"employees_on_linkedin\": None}\n",
    "    try:\n",
    "        soup = get_soup(drv)\n",
    "        a_tags = soup.select(\"a[href*='/company/']\")\n",
    "        if not a_tags:\n",
    "            return result\n",
    "        href = a_tags[0].get(\"href\")\n",
    "        if not href:\n",
    "            return result\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://www.linkedin.com\" + href\n",
    "        result[\"company_page\"] = href\n",
    "        drv.get(href)\n",
    "        wait_css(drv, \"main\", timeout=10)\n",
    "        time.sleep(2)\n",
    "        soup = get_soup(drv)\n",
    "        possible = [\n",
    "            \"div.org-top-card-summary-info-list__info-item\",\n",
    "            \"span.t-normal.t-14.t-black--light\",\n",
    "            \"p.org-top-card__metrics-item\",\n",
    "        ]\n",
    "        text_blobs = []\n",
    "        for sel in possible:\n",
    "            for el in soup.select(sel):\n",
    "                t = el.get_text(\" \", strip=True)\n",
    "                if t:\n",
    "                    text_blobs.append(t)\n",
    "        for t in text_blobs:\n",
    "            if \"followers\" in t.lower() and result[\"followers\"] is None:\n",
    "                result[\"followers\"] = t\n",
    "            if (\"employees\" in t.lower() or \"people on linkedin\" in t.lower()) and result[\"employees_on_linkedin\"] is None:\n",
    "                result[\"employees_on_linkedin\"] = t\n",
    "        return result\n",
    "    except Exception:\n",
    "        return result\n",
    "\n",
    "\n",
    "def scrape_profile_all(drv, profile_url: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        drv.get(profile_url)\n",
    "        wait_css(drv, \"main\", timeout=15)\n",
    "        time.sleep(2)\n",
    "        top = scrape_top_card(drv)\n",
    "        about = scrape_about(drv)\n",
    "        insights = scrape_professional_insights(drv)\n",
    "        skills = scrape_skills(drv, profile_url)\n",
    "        activity = scrape_recent_activity(drv, profile_url)\n",
    "        experience = scrape_experience(drv, profile_url)\n",
    "        company_growth = scrape_company_growth(drv)\n",
    "        out = {\n",
    "            \"Full name\": top.get(\"Full name\", \"Not found\"),\n",
    "            \"Job title\": top.get(\"Job title\", \"Not found\"),\n",
    "            \"Company\": top.get(\"Company\", \"Not found\"),\n",
    "            \"Location\": top.get(\"Location\", \"Not found\"),\n",
    "            \"Professional Insights\": insights,\n",
    "            \"LinkedIn bio\": about,\n",
    "            \"Tech stack\": skills,\n",
    "            \"Recent LinkedIn activity\": activity,\n",
    "            \"Job history changes\": experience,\n",
    "            \"Company growth signals\": company_growth,\n",
    "            \"profile_url\": profile_url,\n",
    "        }\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\" Error scraping profile: {e}\")\n",
    "        return {\n",
    "            \"Full name\": \"Not found\",\n",
    "            \"Job title\": \"Not found\",\n",
    "            \"Company\": \"Not found\",\n",
    "            \"Location\": \"Not found\",\n",
    "            \"Professional Insights\": [],\n",
    "            \"LinkedIn bio\": \"Not found\",\n",
    "            \"Tech stack\": [],\n",
    "            \"Recent LinkedIn activity\": [],\n",
    "            \"Job history changes\": [],\n",
    "            \"Company growth signals\": {\"company_page\": None, \"followers\": None, \"employees_on_linkedin\": None},\n",
    "            \"profile_url\": profile_url,\n",
    "        }\n",
    "\n",
    "\n",
    "def save_json(data: Dict[str, Any], path: str = \"linkedin_profile.json\"):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def save_csv_flat(data: Dict[str, Any], path: str = \"linkedin_profile.csv\"):\n",
    "    row = {\n",
    "        \"Full name\": data.get(\"Full name\", \"\"),\n",
    "        \"Job title\": data.get(\"Job title\", \"\"),\n",
    "        \"Company\": data.get(\"Company\", \"\"),\n",
    "        \"Location\": data.get(\"Location\", \"\"),\n",
    "        \"LinkedIn bio\": data.get(\"LinkedIn bio\", \"\"),\n",
    "        \"Professional Insights\": \"; \".join(data.get(\"Professional Insights\", []) or []),\n",
    "        \"Tech stack\": \"; \".join(data.get(\"Tech stack\", []) or []),\n",
    "        \"Recent LinkedIn activity\": \"; \".join(data.get(\"Recent LinkedIn activity\", []) or []),\n",
    "        \"Job history changes\": json.dumps(data.get(\"Job history changes\", []), ensure_ascii=False),\n",
    "        \"Company growth signals\": json.dumps(data.get(\"Company growth signals\", {}), ensure_ascii=False),\n",
    "        \"profile_url\": data.get(\"profile_url\", \"\"),\n",
    "    }\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = build_driver()\n",
    "        if linkedin_login(driver, EMAIL, PASSWORD):\n",
    "            result = scrape_profile_all(driver, PROFILE_URL)\n",
    "            print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "            save_json(result, \"linkedin_profile.json\")\n",
    "            save_csv_flat(result, \"linkedin_profile.csv\")\n",
    "            print(\"\\n Saved: linkedin_profile.json, linkedin_profile.csv\")\n",
    "        else:\n",
    "            print(\" Scraping aborted due to login failure.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Main execution error: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
